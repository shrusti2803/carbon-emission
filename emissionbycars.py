# -*- coding: utf-8 -*-
"""emissionbycars.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MOl9qqnJPglVnshkK8S-UJDy3s_Klj_t
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import seaborn as sns

from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import ElasticNet

from google.colab import drive
df=pd.read_csv('/Samples.csv')

"""# New Section"""

from google.colab import drive
drive.mount('/content/drive')

df.shape

df.head()

df.info()

df.isnull().values.any()

df.isnull().sum()

from sklearn.preprocessing import LabelEncoder
columns_to_encode = ['MAKE', 'MODEL.1', 'VEHICLE CLASS', 'TRANSMISSION', 'FUEL']
label_encoder = LabelEncoder()
for column in columns_to_encode:
    df[column] = label_encoder.fit_transform(df[column])
df

from sklearn import preprocessing
from scipy.stats import zscore
X = df.drop(['ENGINE_SIZE'],axis=1)
y = df['ENGINE_SIZE']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("Linear Regression model coefficients:",(lr.coef_))
print('LR Mean Square Error:',mean_squared_error(y_test,y_pred_lr))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_lr)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_lr))

ridge = Ridge(alpha=.3)
ridge.fit(X_train,y_train)
y_pred_ridge = ridge.predict(X_test)
print("Ridge model coefficients:",(ridge.coef_))
print('Ridge Mean Square Error:',mean_squared_error(y_test,y_pred_ridge))
print('Ridge Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_ridge)))
print('Ridge Mean absolute error:',mean_absolute_error(y_test,y_pred_ridge))

lasso = Lasso(alpha=0.1)
lasso.fit(X_train,y_train)
y_pred_lasso = lasso.predict(X_test)
print("Lasso model coefficients:",(lasso.coef_))
print('Lasso Mean Square Error:',mean_squared_error(y_test,y_pred_lasso))
print('Lasso Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_lasso)))
print('Lasso Mean absolute error:',mean_absolute_error(y_test,y_pred_lasso))

en = ElasticNet(alpha=1.0, l1_ratio=0.5)
en.fit(X_train,y_train)
y_pred_en = en.predict(X_test)
print("Elastic Net Regression model coefficients:",(en.coef_))
print('EN Mean Square Error:',mean_squared_error(y_test,y_pred_en))
print('EN Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_en)))
print('EN Mean absolute error:',mean_absolute_error(y_test,y_pred_en))

poly = PolynomialFeatures(degree=2, interaction_only=True)
X_poly = poly.fit_transform(X)
X_poly.shape

X_train_poly, X_test_poly, y_train, y_test = train_test_split(X_poly, y, test_size=0.30)

lr = LinearRegression()
lr.fit(X_train_poly, y_train)
y_pred_lr = lr.predict(X_test_poly)
print("Polynomial Regression model coefficients:",(lr.coef_))
print('Mean Square Error:',mean_squared_error(y_test,y_pred_lr))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_lr)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_lr))

from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor

# Initialize a label encoder
#le = LabelEncoder()

# Fit the label encoder on all possible labels
all_labels = np.concatenate((y_pred_lr, y_pred_ridge, y_pred_lasso, y_pred_en, y_pred_lr, y_test))
#le.fit(all_labels)

X = np.array([y_pred_lr, y_pred_ridge, y_pred_lasso, y_pred_en, y_pred_lr]).T
#print(X)
# Train a RandomForest model on the model outputs
meta_learner = RandomForestRegressor()
meta_learner.fit(X, y_test)
ensemble_predictions = meta_learner.predict(X)
print('Mean Square Error:',mean_squared_error(y_test,ensemble_predictions))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,ensemble_predictions)))
print('Mean absolute error:',mean_absolute_error(y_test,ensemble_predictions))
len(ensemble_predictions)

from sklearn.tree import DecisionTreeRegressor
dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
#print("Linear Regression model coefficients:",(dt.coef_))
print('LR Mean Square Error:',mean_squared_error(y_test,y_pred_dt))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_dt)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_dt))
len(y_pred_dt)

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
#print("Linear Regression model coefficients:",(rf.coef_))
print('LR Mean Square Error:',mean_squared_error(y_test,y_pred_rf))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_rf)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_rf))
len(y_pred_rf)

from sklearn.neighbors import KNeighborsRegressor
kn = KNeighborsRegressor()
kn.fit(X_train, y_train)
y_pred_kn = kn.predict(X_test)
#print("Linear Regression model coefficients:",(kn.coef_))
print('LR Mean Square Error:',mean_squared_error(y_test,y_pred_kn))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_kn)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_kn))
len(y_pred_kn)

from sklearn.svm import SVR
sv = SVR()
sv.fit(X_train, y_train)
y_pred_sv = sv.predict(X_test)
#print("Linear Regression model coefficients:",(sv.coef_))
print('LR Mean Square Error:',mean_squared_error(y_test,y_pred_sv))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_sv)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_sv))
len(y_pred_sv)

from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor

# Initialize a label encoder
#le = LabelEncoder()

# Fit the label encoder on all possible labels
all_labels = np.concatenate((y_pred_dt, y_pred_rf, y_pred_kn, y_pred_sv, y_test))
#le.fit(all_labels)

X = np.array([y_pred_dt, y_pred_rf, y_pred_kn, y_pred_sv]).T
#print(X)
# Train a RandomForest model on the model outputs
meta_learner = RandomForestRegressor()
meta_learner.fit(X, y_test)
ensemble_predictions = meta_learner.predict(X)
print('Mean Square Error:',mean_squared_error(y_test,ensemble_predictions))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,ensemble_predictions)))
print('Mean absolute error:',mean_absolute_error(y_test,ensemble_predictions))
len(ensemble_predictions)

!pip install lime
import lime
import lime.lime_tabular

from lime.lime_tabular import LimeTabularExplainer
X = pd.DataFrame(X)
y=pd.DataFrame(y_test)
print(y)
explainer = LimeTabularExplainer(X.values,feature_names=X.columns.values.tolist(),class_names=['TYPE'],mode='regression')
# Now explain a prediction
exp = explainer.explain_instance(X.values[1], meta_learner.predict,num_features=5)

exp.as_pyplot_figure()
from matplotlib import pyplot as plt
plt.tight_layout()

exp.show_in_notebook(show_table=True)

from sklearn import svm
clf = svm.SVR(kernel='linear', C=0.01)
clf.fit(X_train, y_train)
y_pred_svm = clf.predict(X_test)
print('Mean Square Error:',mean_squared_error(y_test,y_pred_svm))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_svm)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_svm))

from lime.lime_tabular import LimeTabularExplainer
#X = pd.DataFrame(X_train)
#y=pd.DataFrame(y_test)
#print(y)
explainer = LimeTabularExplainer(X_train.values,feature_names=X_train.columns.values.tolist(),class_names=['TYPE'],mode='regression')
# Now explain a prediction
exp = explainer.explain_instance(X_test.values[62], clf.predict,num_features=5)

exp.as_pyplot_figure()
from matplotlib import pyplot as plt
plt.tight_layout()

exp.show_in_notebook(show_table=True)

pip install shapash

model2 = RandomForestRegressor(max_depth=5, random_state=42, n_estimators=12)
model2=model2.fit(X_train, y_train)
rf_y_pred = model2.predict(X_test)
rf_y_pred

fi=pd.DataFrame({'Feature': X_train.columns, 'Importance': model2.feature_importances_})
fi.sort_values(by='Importance',ascending=False,ignore_index=True)

from shapash.explainer.smart_explainer import SmartExplainer
xpl = SmartExplainer(model2)
xpl.compile(x=X_test)
xpl.plot.features_importance()
#features_importance.plot(top_n_features=10)

import random
subset = random.choices(X_test.index, k =50)
xpl.plot.features_importance(selection=subset)

xpl.plot.contribution_plot('CYLINDERS')

xpl.plot.local_plot(index=random.choice(X_test.index))

fi=pd.DataFrame({'Feature': X_train.columns, 'Importance': model2.feature_importances_})
fi.sort_values(by='Importance',ascending=False,ignore_index=True)

import shap
row_to_show = 5
data_for_prediction = X_test.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired
data_for_prediction_array = data_for_prediction.values.reshape(1, -1)
model2.predict(data_for_prediction_array)
predictions = model2.predict(data_for_prediction_array)

import shap
explainer = shap.Explainer(model2)
shap_values = explainer(X)
shap_values

fi=pd.DataFrame({'Feature': X.columns, 'Importance': meta_learner.feature_importances_})
fi.sort_values(by='Importance',ascending=False,ignore_index=True)

from shapash.explainer.smart_explainer import SmartExplainer
xpl = SmartExplainer(meta_learner)
xpl.compile(x=X)
xpl.plot.features_importance()

from sklearn.ensemble import GradientBoostingRegressor
gradient_booster = GradientBoostingRegressor(learning_rate=0.1)
gradient_booster.fit(X,y_test)
y_pred_gradboost=gradient_booster.predict(X)
print('Mean Square Error:',mean_squared_error(y_test,y_pred_gradboost))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_gradboost)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_gradboost))

from sklearn.ensemble import AdaBoostRegressor
abc = AdaBoostRegressor()
abc.fit(X, y_test)
y_pred_abc = abc.predict(X)
print('Mean Square Error:',mean_squared_error(y_test,y_pred_abc))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_abc)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_abc))

pip install xgboost

from xgboost import XGBRegressor
model = XGBRegressor(learning_rate=1)
model.fit(X,y_test)
y_pred_xgb=model.predict(X)
print('Mean Square Error:',mean_squared_error(y_test,y_pred_xgb))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_xgb)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_xgb))

!pip install catboost

from catboost import CatBoostRegressor
model1 = CatBoostRegressor()
model1.fit(X,y_test)
y_pred_cat=model1.predict(X)
print('Mean Square Error:',mean_squared_error(y_test,y_pred_cat))
print('Root Mean Square Error:',np.sqrt(mean_squared_error(y_test,y_pred_cat)))
print('Mean absolute error:',mean_absolute_error(y_test,y_pred_cat))